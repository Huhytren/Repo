import numpy as np
import pandas as pd

def create_grouped_triangular_ema(df, id_column, columns, window):
    """
    Computes a Triangular Exponential Moving Average (TEMA) per group.

    Parameters:
    - df: pandas DataFrame with a DateTime column and an ID column.
    - id_column: column name representing the unique ID per group.
    - columns: list of column names to apply TEMA.
    - window: the length of the moving window (e.g., 5).

    Returns:
    - df: DataFrame with additional TEMA columns.
    """
    df = df.copy()
    
    # Create triangular weights (e.g., [0.2, 0.4, 0.6, 0.8, 1.0] for window=5)
    weights = np.linspace(1, window, window)
    weights /= weights.sum()  # Normalize weights to sum to 1

    for col in columns:
        df[f"{col}_tema{window}"] = df.groupby(id_column)[col].transform(
            lambda x: x.rolling(window).apply(lambda y: np.dot(y, weights) if len(y) == window else np.nan, raw=True)
        )
    
    return df

# Example Usage:
df = pd.DataFrame({
    "date": pd.date_range(start="2024-01-01", periods=10, freq="D").tolist() * 2,  # Two IDs
    "id": ["A"] * 10 + ["B"] * 10,
    "price": [10, 12, 14, 13, 15, 17, 16, 18, 20, 19] * 2,
    "volume": [100, 120, 150, 130, 170, 180, 175, 190, 200, 210] * 2
})

df.set_index(["date", "id"], inplace=True)  # Composite key

# Apply the function for a 5-day triangular EMA
columns_to_apply = ["price", "volume"]
window_size = 5

df_with_tema = create_grouped_triangular_ema(df, "id", columns_to_apply, window_size)

import ace_tools as tools
tools.display_dataframe_to_user(name="DataFrame with Grouped Triangular EMA", dataframe=df_with_tema)



import pandas as pd
import numpy as np
import ace_tools as tools  # For displaying data

# Sample Data (Replace with your actual DataFrame)
data = {
    'date': pd.date_range(start='2024-01-01', periods=20, freq='D'),
    'id': np.repeat([1, 2], 10),
    'perf1d': np.random.randn(20)  # Random performance values
}
df = pd.DataFrame(data)

# Define the Forward Triangular EMA function
def forward_triangle_ema(df, window):
    df = df.copy()
    
    # Generate linear weights (triangle shape)
    weights = np.linspace(1, 0, window)  # Decreasing linear weights
    weights /= weights.sum()  # Normalize to sum to 1
    
    # Rolling apply within each group
    def weighted_avg(series):
        if len(series) < window:
            return np.nan  # Avoid short windows at start
        return np.dot(series, weights)

    df['forward_ema'] = (
        df.groupby('id')['perf1d']
        .apply(lambda x: x.shift(-window + 1).rolling(window).apply(weighted_avg, raw=True))
        .reset_index(level=0, drop=True)
    )

    return df

# Apply function with a chosen window size
window_size = 5
df_result = forward_triangle_ema(df, window_size)

# Display the DataFrame
tools.display_dataframe_to_user(name="Forward Triangular EMA", dataframe=df_result)

import pandas as pd
import numpy as np
import ace_tools as tools  # For displaying data

# Sample Data
data = {
    'date': pd.date_range(start='2023-01-01', periods=20, freq='D'),
    'id': np.repeat([1, 2], 10),
    'perf1d': np.random.randn(20)  # Random performance values
}
df = pd.DataFrame(data)

# Define Function to Compute Weighted Scalar Product
def weighted_scalar_product(series, half_life):
    if len(series) == 0:
        return np.nan  # Handle empty cases
    
    # Compute weights with log-space exponential decay
    weights = np.logspace(0, -len(series) + 1, num=len(series), base=np.exp(-1 / half_life))
    weights /= weights.sum()  # Normalize
    
    # Compute scalar product
    return np.dot(series.values[::-1], weights)  # Reverse to give more weight to recent values

# Parameters
half_life = 3  # Half-life decay rate

# Apply Transform with Expanding Window
df['wema_expanding'] = df.groupby('id')['perf1d'].transform(lambda x: x.expanding().apply(weighted_scalar_product, args=(half_life,), raw=True))

# Display the DataFrame
tools.display_dataframe_to_user(name="WEMA with Expanding Transform", dataframe=df)


import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

def orthogonalize_target(df, date_col, target_1, target_2):
    """
    Orthogonalizes daily target_1 with respect to target_2.

    Parameters:
    df (pd.DataFrame): DataFrame containing the dataset.
    date_col (str): Column name for date.
    target_1 (str): Column name for target 1 (dependent variable).
    target_2 (str): Column name for target 2 (independent variable).

    Returns:
    pd.DataFrame: DataFrame with an additional column for the orthogonalized target_1.
    """
    df = df.copy()
    df["target_1_orthogonal"] = np.nan

    for date, group in df.groupby(date_col):
        if len(group) > 1:  # Perform regression only if there are multiple observations
            X = group[[target_2]].values
            y = group[target_1].values
            model = LinearRegression().fit(X, y)
            residuals = y - model.predict(X)
            df.loc[group.index, "target_1_orthogonal"] = residuals
        else:
            df.loc[group.index, "target_1_orthogonal"] = group[target_1].values  # No change if only one sample

    return df
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

def orthogonalize_target(df, date_col, target_1, target_2):
    """
    Orthogonalizes daily target_1 with respect to target_2.

    Parameters:
    df (pd.DataFrame): DataFrame containing the dataset.
    date_col (str): Column name for date.
    target_1 (str): Column name for target 1 (dependent variable).
    target_2 (str): Column name for target 2 (independent variable).

    Returns:
    pd.DataFrame: DataFrame with an additional column for the orthogonalized target_1.
    """
    df = df.copy()
    
    def orthogonalize(group):
        if len(group) > 1:
            X = group[[target_2]].values
            y = group[target_1].values
            model = LinearRegression().fit(X, y)
            return y - model.predict(X)
        return group[target_1]  # No change if only one observation
    
    df["target_1_orthogonal"] = df.groupby(date_col)[[target_1, target_2]].transform(orthogonalize)
    
    return df
